# Anthropic API Proxy (repurposed) ğŸ”„

**A simple proxy server for Anthropic API using LiteLLM with Langfuse logging.** ğŸ¤

A proxy server that accepts Anthropic API requests, forwards them through LiteLLM, and logs all interactions to Langfuse for observability and analytics. ğŸŒ‰


![Anthropic API Proxy (repurposed)](pic2.jpg)

## Quick Start âš¡

### Prerequisites

- Anthropic API key ğŸ”‘
- Langfuse account and API keys (for logging) ğŸ“Š
- [uv](https://github.com/astral-sh/uv) installed.

### Setup ğŸ› ï¸

1. **Clone this repository**:
   ```bash
   git clone https://github.com/teremterem/claude-code-openai.git
   cd claude-code-openai
   ```

2. **Install uv** (if you haven't already):
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```
   *(`uv` will handle dependencies based on `pyproject.toml` when you run the server)*

3. **Configure Environment Variables**:
   Create a `.env` file:
   ```bash
   touch .env
   ```
   Edit `.env` and add your API keys:
   ```dotenv
   ANTHROPIC_API_KEY=your-anthropic-api-key-here
   LANGFUSE_PUBLIC_KEY=your-langfuse-public-key
   LANGFUSE_SECRET_KEY=your-langfuse-secret-key
   LANGFUSE_HOST=https://cloud.langfuse.com
   ```

4. **Run the server**:
   ```bash
   uv run uvicorn server:app --host 0.0.0.0 --port 8082 --reload
   ```
   *(`--reload` is optional, for development)*

### Using with Claude Code ğŸ®

1. **Install Claude Code** (if you haven't already):
   ```bash
   npm install -g @anthropic-ai/claude-code
   ```

2. **Connect to your proxy**:
   ```bash
   ANTHROPIC_BASE_URL=http://localhost:8082 claude
   ```

3. **That's it!** Your Claude Code client will now use Anthropic models through the proxy. ğŸ¯

## Supported Models

The proxy supports all Anthropic models available through LiteLLM, including:
- claude-3-5-sonnet-20241022
- claude-3-5-haiku-20241022
- claude-3-opus-20240229
- And other Anthropic models supported by LiteLLM

## How It Works ğŸ§©

This proxy works by:

1. **Receiving requests** in Anthropic's API format ğŸ“¥
2. **Converting** the requests to LiteLLM format ğŸ”„
3. **Sending** the request to Anthropic via LiteLLM ğŸ“¤
4. **Logging** all interactions to Langfuse for observability ğŸ“Š
5. **Converting** the response back to Anthropic format ğŸ”„
6. **Returning** the formatted response to the client âœ…

The proxy handles both streaming and non-streaming responses, maintaining compatibility with all Claude clients while providing comprehensive logging and analytics through Langfuse. ğŸŒŠ

## Langfuse Integration ğŸ“Š

All API interactions are automatically logged to Langfuse, providing:
- Request/response tracking
- Performance metrics
- Usage analytics
- Error monitoring
- Token consumption tracking

Configure your Langfuse credentials in the `.env` file to enable logging.

## Contributing ğŸ¤

Contributions are welcome! Please feel free to submit a Pull Request. ğŸ
